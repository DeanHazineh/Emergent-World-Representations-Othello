{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import EWOthello.utils.plot_helpers as plt_util\n",
    "from EWOthello.data.othello import *\n",
    "from EWOthello.mingpt.dataset import ProbingDataset, CharDataset # AK's mingpt data child \n",
    "from EWOthello.mingpt.model import GPT, GPTConfig, GPTforProbing # AKs and KLi GPT models\n",
    "#from EWOthello.mingpt.trainer import Trainer, TrainerConfig # AKs GPT trainer\n",
    "from EWOthello.mingpt.probe_trainer import Trainer, TrainerConfig\n",
    "from EWOthello.mingpt.utils import set_seed, sample # AKs helpers for sampling predictions\n",
    "from EWOthello.mingpt.probe_model import BatteryProbeClassification, BatteryProbeClassificationTwoLayer\n",
    "set_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 392.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created has 1000 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "othello = get(ood_num=int(1e3), data_root=None)\n",
    "train_dataset = CharDataset(othello) \n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a truncated GPT model (with random or pre-trained weights) that will be used to get activations from after attention modules\n",
    "# you can define if we want layernorm applied at output or not (defaults to not)\n",
    "model_probe = GPTforProbing(mconf, probe_layer=2)\n",
    "\n",
    "mode = \"synthetic\"\n",
    "# Apply the GPT model weights\n",
    "if mode==\"random\":\n",
    "    model_probe.apply(model._init_weights)\n",
    "else:\n",
    "    path = \"../EWOthello/ckpts/gpt_championship.ckpt\" if mode==\"championship\" else \"../EWOthello/ckpts/gpt_synthetic.ckpt\"\n",
    "    model_probe.load_state_dict(torch.load(path))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model_probe = model_probe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 59, 61])\n",
      "torch.Size([1, 59, 512])\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "x = torch.unsqueeze(x, 0)\n",
    "\n",
    "model_out_logits = model(x.to(device))[0]\n",
    "print(model_out_logits.shape)\n",
    "\n",
    "probe_activation = model_probe(x.to(device))\n",
    "print(probe_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "59\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## mini-example before getting to the main part\n",
    "\n",
    "# Grab a single game sequence index vector x\n",
    "loader = DataLoader(train_dataset, shuffle=False, pin_memory=True, batch_size=1, num_workers=1)\n",
    "it = iter(loader)\n",
    "x,_ = next(it)\n",
    "\n",
    "act_container = []\n",
    "property_container = []\n",
    "\n",
    "# Note the sequence length and get tbf = index to tokens\n",
    "tbf = [train_dataset.itos[_] for _ in x.tolist()[0]] # Index to string (converts the sequence to board state)\n",
    "valid_until = tbf.index(-100) if -100 in tbf else 999 # Get true length of game sequence (excluding filler token -100) \n",
    "# I dont think this valid_until treatment is correct but okay...\n",
    "\n",
    "# Get the activation output features\n",
    "act = model_probe(x.to(device))[0].detach().cpu()  # [block_size, f]\n",
    "act = [_[0] for _ in act.split(1, dim=0)[:valid_until]]  # list of activations after the n-th layer block\n",
    "print(len(act))\n",
    "act_container.extend(act)\n",
    "\n",
    "# Get the board state\n",
    "a = OthelloBoardState()\n",
    "properties = a.get_gt(tbf[:valid_until], \"get_state\") \n",
    "property_container.extend(properties)\n",
    "print(len(properties)) # list of 8*8 boardstates for each subsequence up until the full sequence length\n",
    "num = 4\n",
    "fig = plt.figure()\n",
    "ax = plt_util.addAxis(fig, 1,num)\n",
    "for i in range(num):\n",
    "    board = np.reshape(np.array(properties[i]), [8,8])\n",
    "    ax[i].imshow(board)\n",
    "\n",
    "# Get the \"age\"\n",
    "a = OthelloBoardState()\n",
    "ages = a.get_gt(tbf[:valid_until], \"get_age\")  # [block_size, ]\n",
    "fig = plt.figure()\n",
    "ax = plt_util.addAxis(fig, 1,num)\n",
    "for i in range(num):\n",
    "    out = np.reshape(np.array(ages[i]), [8,8])\n",
    "    ax[i].imshow(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 282.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59000 59000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 545.90it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(train_dataset, shuffle=False, pin_memory=True, batch_size=1, num_workers=1)\n",
    "act_container = []\n",
    "property_container = []\n",
    "for x, y in tqdm(loader, total=len(loader)):\n",
    "    tbf = [train_dataset.itos[_] for _ in x.tolist()[0]] # Index to string (converts the sequence to board state)\n",
    "    valid_until = tbf.index(-100) if -100 in tbf else 999\n",
    "    a = OthelloBoardState()\n",
    "    properties = a.get_gt(tbf[:valid_until], \"get_state\")  # [block_size, ]\n",
    "    \n",
    "    act = model_probe(x.to(device))[0].detach().cpu()  # [block_size, f]\n",
    "    act_container.extend([_[0] for _ in act.split(1, dim=0)[:valid_until]])\n",
    "    property_container.extend(properties)\n",
    "\n",
    "age_container = []\n",
    "for x, y in tqdm(loader, total=len(loader)):\n",
    "    tbf = [train_dataset.itos[_] for _ in x.tolist()[0]]\n",
    "    valid_until = tbf.index(-100) if -100 in tbf else 999\n",
    "    a = OthelloBoardState()\n",
    "    ages = a.get_gt(tbf[:valid_until], \"get_age\")  # [block_size, ]\n",
    "    age_container.extend(ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59000 pairs loaded...\n",
      "983159 1770000 1022841\n",
      "[189577, 223213, 217427, 191617, 186641, 165972, 161663, 144914, 141064, 127430, 124000, 112471, 109375, 99693, 96799, 88768, 86170, 79419, 76972, 71012, 68751, 63683, 61570, 57135, 55208, 51450, 49606, 46364, 44672, 41859, 40265, 37714, 36183, 33910, 32449, 30398, 28992, 27164, 25799, 24111, 22795, 21269, 20025, 18623, 17407, 16065, 14931, 13687, 12576, 11414, 10285, 9180, 8122, 7073, 6036, 5015, 4010, 3004, 2003, 1000]\n",
      "torch.Size([512])\n",
      "512 64 64\n"
     ]
    }
   ],
   "source": [
    "probing_dataset = ProbingDataset(act_container, property_container, age_container)\n",
    "probe = BatteryProbeClassification(device, probe_class=3, num_task=64)\n",
    "\n",
    "# probing dataset returns activation, y, and age\n",
    "# Porbe at the dataloader\n",
    "it = iter(probing_dataset)\n",
    "act, y, age = next(it)\n",
    "\n",
    "print(act.shape)\n",
    "print(len(act), len(y), len(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decayed: {'proj.bias', 'proj.weight'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss 0.36498; lr 1.00e-03; train acc 77.67%: 100%|██████████| 47/47 [00:00<00:00, 69.38it/s]\n",
      "epoch 2: train loss 0.36259; lr 1.00e-03; train acc 77.86%: 100%|██████████| 47/47 [00:00<00:00, 74.06it/s]\n",
      "epoch 3: train loss 0.36206; lr 7.50e-04; train acc 77.95%: 100%|██████████| 47/47 [00:00<00:00, 72.62it/s]\n",
      "epoch 4: train loss 0.36146; lr 5.63e-04; train acc 78.03%: 100%|██████████| 47/47 [00:00<00:00, 77.63it/s]\n",
      "epoch 5: train loss 0.36014; lr 4.22e-04; train acc 78.08%: 100%|██████████| 47/47 [00:00<00:00, 73.67it/s]\n",
      "epoch 6: train loss 0.36004; lr 3.16e-04; train acc 78.12%: 100%|██████████| 47/47 [00:00<00:00, 75.81it/s]\n",
      "epoch 7: train loss 0.35921; lr 2.37e-04; train acc 78.15%: 100%|██████████| 47/47 [00:00<00:00, 73.38it/s]\n",
      "epoch 8: train loss 0.36026; lr 1.78e-04; train acc 78.17%: 100%|██████████| 47/47 [00:00<00:00, 73.11it/s]\n",
      "epoch 9: train loss 0.36032; lr 1.33e-04; train acc 78.18%: 100%|██████████| 47/47 [00:00<00:00, 77.03it/s]\n",
      "epoch 10: train loss 0.35878; lr 1.00e-04; train acc 78.20%: 100%|██████████| 47/47 [00:00<00:00, 74.54it/s]\n",
      "epoch 11: train loss 0.35948; lr 7.51e-05; train acc 78.20%: 100%|██████████| 47/47 [00:00<00:00, 74.35it/s]\n",
      "epoch 12: train loss 0.35922; lr 5.63e-05; train acc 78.21%: 100%|██████████| 47/47 [00:00<00:00, 71.79it/s]\n",
      "epoch 13: train loss 0.35862; lr 4.22e-05; train acc 78.22%: 100%|██████████| 47/47 [00:00<00:00, 74.70it/s]\n",
      "epoch 14: train loss 0.35815; lr 3.17e-05; train acc 78.22%: 100%|██████████| 47/47 [00:00<00:00, 77.84it/s]\n",
      "epoch 15: train loss 0.35956; lr 2.38e-05; train acc 78.22%: 100%|██████████| 47/47 [00:00<00:00, 73.99it/s]\n",
      "epoch 16: train loss 0.35898; lr 1.78e-05; train acc 78.22%: 100%|██████████| 47/47 [00:00<00:00, 75.44it/s]\n",
      "epoch 17: train loss 0.35857; lr 1.34e-05; train acc 78.22%: 100%|██████████| 47/47 [00:00<00:00, 72.66it/s]\n",
      "epoch 18: train loss 0.35875; lr 1.00e-05; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.27it/s]\n",
      "epoch 19: train loss 0.35964; lr 7.52e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 74.57it/s]\n",
      "epoch 20: train loss 0.35933; lr 5.64e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 70.98it/s]\n",
      "epoch 21: train loss 0.35870; lr 4.23e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.64it/s]\n",
      "epoch 22: train loss 0.35866; lr 3.17e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.18it/s]\n",
      "epoch 23: train loss 0.35860; lr 2.38e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 76.35it/s]\n",
      "epoch 24: train loss 0.35855; lr 1.78e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 74.37it/s]\n",
      "epoch 25: train loss 0.35847; lr 1.34e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 74.59it/s]\n",
      "epoch 26: train loss 0.35955; lr 1.00e-06; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 71.17it/s]\n",
      "epoch 27: train loss 0.35899; lr 7.53e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.47it/s]\n",
      "epoch 28: train loss 0.35920; lr 5.64e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 77.15it/s]\n",
      "epoch 29: train loss 0.35890; lr 4.23e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.74it/s]\n",
      "epoch 30: train loss 0.35933; lr 3.17e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 77.27it/s]\n",
      "epoch 31: train loss 0.35912; lr 2.38e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 77.68it/s]\n",
      "epoch 32: train loss 0.35895; lr 1.79e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.73it/s]\n",
      "epoch 33: train loss 0.35933; lr 1.34e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 76.87it/s]\n",
      "epoch 34: train loss 0.35868; lr 1.00e-07; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 76.19it/s]\n",
      "epoch 35: train loss 0.35877; lr 7.53e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.83it/s]\n",
      "epoch 36: train loss 0.35877; lr 5.65e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 70.63it/s]\n",
      "epoch 37: train loss 0.35875; lr 4.24e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.38it/s]\n",
      "epoch 38: train loss 0.35906; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 76.56it/s]\n",
      "epoch 39: train loss 0.35882; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.87it/s]\n",
      "epoch 40: train loss 0.35850; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 74.20it/s]\n",
      "epoch 41: train loss 0.35888; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 69.76it/s]\n",
      "epoch 42: train loss 0.35923; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 69.07it/s]\n",
      "epoch 43: train loss 0.35842; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.24it/s]\n",
      "epoch 44: train loss 0.35912; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.48it/s]\n",
      "epoch 45: train loss 0.35870; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.35it/s]\n",
      "epoch 46: train loss 0.35866; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 74.22it/s]\n",
      "epoch 47: train loss 0.35869; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 73.54it/s]\n",
      "epoch 48: train loss 0.35925; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.20it/s]\n",
      "epoch 49: train loss 0.35894; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 75.28it/s]\n",
      "epoch 50: train loss 0.35894; lr 3.18e-08; train acc 78.23%: 100%|██████████| 47/47 [00:00<00:00, 72.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(probing_dataset))\n",
    "test_size = len(probing_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(probing_dataset, [train_size, test_size])\n",
    "sampler = None\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, sampler=sampler, pin_memory=True, batch_size=128, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, pin_memory=True, batch_size=128, num_workers=1)\n",
    "\n",
    "savepath = \"../EWOthello/ckpts/\"\n",
    "max_epochs = 50\n",
    "t_start = time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=1024,\n",
    "    learning_rate=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    lr_decay=True,\n",
    "    warmup_tokens=len(train_dataset) * 5,\n",
    "    final_tokens=len(train_dataset) * max_epochs,\n",
    "    num_workers=4,\n",
    "    weight_decay=0.0,\n",
    "    ckpt_path=savepath + f\"probe_test_at{t_start}.ckpt\", \n",
    ")\n",
    "trainer = Trainer(probe, train_dataset, test_dataset, tconf)\n",
    "trainer.train(prt=True)\n",
    "trainer.save_traces()\n",
    "trainer.save_checkpoint()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent_world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
