{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deanhazineh/anaconda3/envs/emergent_world/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import EWOthello.utils.plot_helpers as plt_util\n",
    "from EWOthello.data.othello import *\n",
    "from EWOthello.mingpt.dataset import ProbingDataset, CharDataset # AK's mingpt data child \n",
    "from EWOthello.mingpt.model import GPT, GPTConfig, GPTforProbing, GPTforProbing_v2\n",
    "from EWOthello.mingpt.utils import set_seed\n",
    "set_seed(44)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GPT_Othello(game_dataset, n_layers, n_heads, batch_size=64, num_epochs=100, train_to=120000, train_ratio=0.8, val_legal_stepsize=10, num_val=500, save_at_steps=20, learning_rate=1e-4, lr_schedule=False, verbose=False):\n",
    "    model_name = f\"GPT_Synthetic_{n_layers}Layers_{n_heads}Heads\"\n",
    "    savepath = f\"../EWOthello/ckpts/Dean_GPTv2_Synthetic_{n_layers}L{n_heads}H/\"\n",
    "    mconf = GPTConfig(vocab_size=61, block_size=59, n_layer=n_layers, n_head=n_heads, n_embd=512)\n",
    "    model = GPT(mconf)\n",
    "    model = model.to(device)\n",
    "\n",
    "    training_data = game_dataset\n",
    "    train_size = int(train_ratio * len(training_data))\n",
    "    test_size = len(training_data) - train_size - num_val\n",
    "    val_size = num_val\n",
    "\n",
    "    train_dataset, test_dataset, val_dataset = random_split(training_data, [train_size, test_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "    test_iter = iter(test_dataloader)\n",
    "\n",
    "    warm_up_tokens = train_size*59\n",
    "    warm_up_max = train_size*59*num_epochs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    training_loss_history = []\n",
    "    testing_loss_history = []\n",
    "    perc_legal_games = []\n",
    "    tokens_processed = 0\n",
    "    num_steps = 0 if len(training_loss_history)==0 else len(training_loss_history)\n",
    "\n",
    "    print(f\"Training Model Name: {model_name}; training/test set size {train_size}/{test_size}; validation games {val_size}\")\n",
    "    if not os.path.exists(savepath):\n",
    "        os.mkdir(savepath)\n",
    "\n",
    "    if os.path.exists(savepath + model_name + \".ckpt\"):\n",
    "        model.load_state_dict(torch.load(savepath + model_name + \".ckpt\"))\n",
    "        print(f\"Loaded model checkpopint from {savepath + model_name + '.ckpt'}\")\n",
    "        with open(savepath + model_name + \".pickle\", 'rb') as fhandle:\n",
    "            training_history = pickle.load(fhandle)\n",
    "            training_loss_history = training_history[\"training_loss\"]\n",
    "            testing_loss_history = training_history[\"testing_loss\"]\n",
    "            perc_legal_games = training_history[\"val_legal_perc\"]\n",
    "            tokens_processed = training_history[\"tokens_processed\"]\n",
    "\n",
    "    num_steps = 0 if len(training_loss_history)==0 else len(training_loss_history)\n",
    "    print(f\"Training steps Current: {num_steps}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        i = 0\n",
    "        for (x,y) in tqdm(train_dataloader):\n",
    "            if num_steps > train_to:\n",
    "                break\n",
    "\n",
    "            # Run update training step SGD\n",
    "            model.train()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits, loss = model(x, y)\n",
    "            train_loss = loss.item()\n",
    "            training_loss_history.append(train_loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate if using non-fixed scheduler\n",
    "            if lr_schedule:\n",
    "                tokens_processed += (y>=0).sum()\n",
    "                if tokens_processed < warm_up_tokens:\n",
    "                    lr_mult = tokens_processed / warm_up_tokens\n",
    "                else:\n",
    "                    progress = (tokens_processed - warm_up_tokens) / (warm_up_max-warm_up_tokens)\n",
    "                    lr_mult = max(0.1, 0.5*(1 + math.cos(math.pi*progress)))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = learning_rate * lr_mult\n",
    "\n",
    "            ## Compute the error on test batch\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    x,y = next(test_iter)\n",
    "                except:\n",
    "                    test_iter = iter(test_dataloader)\n",
    "                    x,y = next(test_iter)\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)        \n",
    "                logits, loss = model(x,y)\n",
    "                test_loss = loss.item()\n",
    "                testing_loss_history.append(test_loss)\n",
    "\n",
    "            ## After a certain number of steps, calculate percent of legal moves the model plays\n",
    "            if i % val_legal_stepsize == 0:\n",
    "                legal_moves_played = 0\n",
    "                for _, (x,y) in enumerate(val_dataloader):\n",
    "                    x = x.to(device)\n",
    "                    logits,_ = model(x)\n",
    "                    moves = torch.argmax(logits, dim=2)[0]\n",
    "                    moves = moves.detach().cpu().numpy()\n",
    "                    x = x.detach().cpu().numpy()\n",
    "                    for len_partial in range(59):\n",
    "                        partial_x = list(x[0,:len_partial+1])\n",
    "                        partial_x.append(moves[len_partial])\n",
    "                        game_string = [training_data.itos[int(move_idx)] for move_idx in partial_x]\n",
    "                        try:\n",
    "                            OthelloBoardState().update(game_string, prt=False)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        else:\n",
    "                            legal_moves_played +=1        \n",
    "                perc_legal_games.append(legal_moves_played/val_size/59)\n",
    "            else:\n",
    "                perc_legal_games.append(None)\n",
    "        \n",
    "            ## Save/print\n",
    "            num_steps +=1\n",
    "            i = i + 1\n",
    "            if (i+1) % save_at_steps == 0:\n",
    "                #print(\"Saving Model Checkpoint\")\n",
    "                torch.save(model.state_dict(), savepath + model_name + \".ckpt\")\n",
    "                training_history = {\"training_loss\": training_loss_history, \"testing_loss\": testing_loss_history, \"val_legal_perc\": perc_legal_games, \"tokens_processed\": tokens_processed}\n",
    "                with open(savepath + model_name + \".pickle\", 'wb') as fhandle:\n",
    "                    pickle.dump(training_history, fhandle)\n",
    "        \n",
    "        # Save after each epoch also\n",
    "        torch.save(model.state_dict(), savepath + model_name + \".ckpt\")\n",
    "        training_history = {\"training_loss\": training_loss_history, \"testing_loss\": testing_loss_history, \"val_legal_perc\": perc_legal_games, \"tokens_processed\": tokens_processed}\n",
    "        with open(savepath + model_name + \".pickle\", 'wb') as fhandle:\n",
    "            pickle.dump(training_history, fhandle)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num files: 230; Use_num: 200\n",
      "['gen10e5__20220324_165952.pickle', 'gen10e5__20220324_154919.pickle', 'gen10e5__20220324_164123.pickle', 'gen10e5__20220324_154043.pickle', 'gen10e5__20220324_155251.pickle', 'gen10e5__20220324_160016.pickle', 'gen10e5__20220324_165748.pickle', 'gen10e5__20220324_154002.pickle', 'gen10e5__20220324_155241.pickle', 'gen10e5__20220324_165707.pickle', 'gen10e5__20220324_160046.pickle', 'gen10e5__20220324_154811.pickle', 'gen10e5__20220324_154806.pickle', 'gen10e5__20220324_162637.pickle', 'gen10e5__20220324_154048.pickle', 'gen10e5__20220324_155439.pickle', 'gen10e5__20220324_155255.pickle', 'gen10e5__20220324_154235.pickle', 'gen10e5__20220324_160049.pickle', 'gen10e5__20220324_154032.pickle', 'gen10e5__20220324_164213.pickle', 'gen10e5__20220324_155245.pickle', 'gen10e5__20220324_154722.pickle', 'gen10e5__20220324_165841.pickle', 'gen10e5__20220324_162202.pickle', 'gen10e5__20220324_154533.pickle', 'gen10e5__20220324_164648.pickle', 'gen10e5__20220324_170049.pickle', 'gen10e5__20220324_160017.pickle', 'gen10e5__20220324_155959.pickle', 'gen10e5__20220324_162758.pickle', 'gen10e5__20220324_170016.pickle', 'gen10e5__20220324_154545.pickle', 'gen10e5__20220324_154150.pickle', 'gen10e5__20220324_154104.pickle', 'gen10e5__20220324_155240.pickle', 'gen10e5__20220324_155905.pickle', 'gen10e5__20220324_161539.pickle', 'gen10e5__20220324_163829.pickle', 'gen10e5__20220324_155404.pickle', 'gen10e5__20220324_154100.pickle', 'gen10e5__20220324_155730.pickle', 'gen10e5__20220324_170238.pickle', 'gen10e5__20220324_155824.pickle', 'gen10e5__20220324_154434.pickle', 'gen10e5__20220324_170041.pickle', 'gen10e5__20220324_155947.pickle', 'gen10e5__20220324_155351.pickle', 'gen10e5__20220324_162312.pickle', 'gen10e5__20220324_170335.pickle', 'gen10e5__20220324_164426.pickle', 'gen10e5__20220324_160129.pickle', 'gen10e5__20220324_162533.pickle', 'gen10e5__20220324_154325.pickle', 'gen10e5__20220324_154830.pickle', 'gen10e5__20220324_155353.pickle', 'gen10e5__20220324_162048.pickle', 'gen10e5__20220324_165957.pickle', 'gen10e5__20220324_155732.pickle', 'gen10e5__20220324_155621.pickle', 'gen10e5__20220324_162524.pickle', 'gen10e5__20220324_162040.pickle', 'gen10e5__20220324_154356.pickle', 'gen10e5__20220324_170043.pickle', 'gen10e5__20220324_155422.pickle', 'gen10e5__20220324_162706.pickle', 'gen10e5__20220324_155340.pickle', 'gen10e5__20220324_154036.pickle', 'gen10e5__20220324_164312.pickle', 'gen10e5__20220324_162151.pickle', 'gen10e5__20220324_155812.pickle', 'gen10e5__20220324_170123.pickle', 'gen10e5__20220324_161346.pickle', 'gen10e5__20220324_160218.pickle', 'gen10e5__20220324_155718.pickle', 'gen10e5__20220324_154720.pickle', 'gen10e5__20220324_164257.pickle', 'gen10e5__20220324_162251.pickle', 'gen10e5__20220324_154404.pickle', 'gen10e5__20220324_165341.pickle', 'gen10e5__20220324_165507.pickle', 'gen10e5__20220324_162458.pickle', 'gen10e5__20220324_155817.pickle', 'gen10e5__20220324_170426.pickle', 'gen10e5__20220324_154422.pickle', 'gen10e5__20220324_161941.pickle', 'gen10e5__20220324_154729.pickle', 'gen10e5__20220324_155446.pickle', 'gen10e5__20220324_155230.pickle', 'gen10e5__20220324_155508.pickle', 'gen10e5__20220324_170032.pickle', 'gen10e5__20220324_162148.pickle', 'gen10e5__20220324_162212.pickle', 'gen10e5__20220324_164805.pickle', 'gen10e5__20220324_162422.pickle', 'gen10e5__20220324_154223.pickle', 'gen10e5__20220324_155637.pickle', 'gen10e5__20220324_154803.pickle', 'gen10e5__20220324_170403.pickle', 'gen10e5__20220324_155735.pickle', 'gen10e5__20220324_161808.pickle', 'gen10e5__20220324_163810.pickle', 'gen10e5__20220324_160052.pickle', 'gen10e5__20220324_160102.pickle', 'gen10e5__20220324_154038.pickle', 'gen10e5__20220324_154210.pickle', 'gen10e5__20220324_155447.pickle', 'gen10e5__20220324_155946.pickle', 'gen10e5__20220324_154425.pickle', 'gen10e5__20220324_160053.pickle', 'gen10e5__20220324_155804.pickle', 'gen10e5__20220324_155722.pickle', 'gen10e5__20220324_155720.pickle', 'gen10e5__20220324_170457.pickle', 'gen10e5__20220324_161450.pickle', 'gen10e5__20220324_155711.pickle', 'gen10e5__20220324_154650.pickle', 'gen10e5__20220324_165736.pickle', 'gen10e5__20220324_155234.pickle', 'gen10e5__20220324_154058.pickle', 'gen10e5__20220324_162518.pickle', 'gen10e5__20220324_170148.pickle', 'gen10e5__20220324_155814.pickle', 'gen10e5__20220324_154128.pickle', 'gen10e5__20220324_155823.pickle', 'gen10e5__20220324_162745.pickle', 'gen10e5__20220324_155846.pickle', 'gen10e5__20220324_164435.pickle', 'gen10e5__20220324_154317.pickle', 'gen10e5__20220324_154344.pickle', 'gen10e5__20220324_171520.pickle', 'gen10e5__20220324_163802.pickle', 'gen10e5__20220324_164351.pickle', 'gen10e5__20220324_155639.pickle', 'gen10e5__20220324_154447.pickle', 'gen10e5__20220324_154331.pickle', 'gen10e5__20220324_154407.pickle', 'gen10e5__20220324_155642.pickle', 'gen10e5__20220324_154829.pickle', 'gen10e5__20220324_154144.pickle', 'gen10e5__20220324_154738.pickle', 'gen10e5__20220324_154121.pickle', 'gen10e5__20220324_160048.pickle', 'gen10e5__20220324_154217.pickle', 'gen10e5__20220324_162650.pickle', 'gen10e5__20220324_154814.pickle', 'gen10e5__20220324_162257.pickle', 'gen10e5__20220324_162526.pickle', 'gen10e5__20220324_164314.pickle', 'gen10e5__20220324_162622.pickle', 'gen10e5__20220324_154801.pickle', 'gen10e5__20220324_164644.pickle', 'gen10e5__20220324_164334.pickle', 'gen10e5__20220324_161838.pickle', 'gen10e5__20220324_162320.pickle', 'gen10e5__20220324_161410.pickle', 'gen10e5__20220324_155726.pickle', 'gen10e5__20220324_165548.pickle', 'gen10e5__20220324_155525.pickle', 'gen10e5__20220324_164319.pickle', 'gen10e5__20220324_155706.pickle', 'gen10e5__20220324_162246.pickle', 'gen10e5__20220324_155719.pickle', 'gen10e5__20220324_170300.pickle', 'gen10e5__20220324_160040.pickle', 'gen10e5__20220324_163753.pickle', 'gen10e5__20220324_160152.pickle', 'gen10e5__20220324_155605.pickle', 'gen10e5__20220324_161911.pickle', 'gen10e5__20220324_160111.pickle', 'gen10e5__20220324_165533.pickle', 'gen10e5__20220324_162153.pickle', 'gen10e5__20220324_162152.pickle', 'gen10e5__20220324_155838.pickle', 'gen10e5__20220324_160512.pickle', 'gen10e5__20220324_155338.pickle', 'gen10e5__20220324_165701.pickle', 'gen10e5__20220324_161429.pickle', 'gen10e5__20220324_154021.pickle', 'gen10e5__20220324_154649.pickle', 'gen10e5__20220324_162129.pickle', 'gen10e5__20220324_162430.pickle', 'gen10e5__20220324_155756.pickle', 'gen10e5__20220324_162502.pickle', 'gen10e5__20220324_155938.pickle', 'gen10e5__20220324_165704.pickle', 'gen10e5__20220324_163039.pickle', 'gen10e5__20220324_154148.pickle', 'gen10e5__20220324_154351.pickle', 'gen10e5__20220324_154403.pickle', 'gen10e5__20220324_165907.pickle', 'gen10e5__20220324_162331.pickle', 'gen10e5__20220324_154428.pickle', 'gen10e5__20220324_154248.pickle', 'gen10e5__20220324_155041.pickle', 'gen10e5__20220324_155319.pickle', 'gen10e5__20220324_154755.pickle', 'gen10e5__20220324_153933.pickle', 'gen10e5__20220324_155047.pickle', 'gen10e5__20220324_170059.pickle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mem Used: 12.78 GB: 100%|██████████| 200/200 [00:52<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n",
      "Deduplicating finished with 19996732 games left\n",
      "Using 20 million for training, 0 for validation\n",
      "Dataset created has 19996732 sequences, 61 unique words.\n"
     ]
    }
   ],
   "source": [
    "othello = get(ood_num=-1, data_root=None, num_preload=200) \n",
    "game_dataset = CharDataset(othello) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Name: GPT_Synthetic_8Layers_8Heads; training/test set size 15997385/3998847; validation games 500\n",
      "Loaded model checkpopint from ../EWOthello/ckpts/Dean_GPTv2_Synthetic_8L8H/GPT_Synthetic_8Layers_8Heads.ckpt\n",
      "Training steps Current: 84112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 84/31245 [00:53<3:46:01,  2.30it/s] "
     ]
    }
   ],
   "source": [
    "for num_layers in [8]:\n",
    "    train_GPT_Othello(game_dataset, n_layers=num_layers, n_heads=8, batch_size=512, learning_rate=1e-4, lr_schedule=False, num_epochs=4, train_to=120000, val_legal_stepsize=1000, save_at_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Name: GPT_Synthetic_8Layers_1Heads; training/test set size 7998824/1999206; validation games 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15623/15623 [1:54:29<00:00,  2.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Name: GPT_Synthetic_4Layers_1Heads; training/test set size 7998824/1999206; validation games 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15623/15623 [1:03:11<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Name: GPT_Synthetic_2Layers_1Heads; training/test set size 7998824/1999206; validation games 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15623/15623 [38:01<00:00,  6.85it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Name: GPT_Synthetic_1Layers_1Heads; training/test set size 7998824/1999206; validation games 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15623/15623 [25:21<00:00, 10.27it/s]  \n"
     ]
    }
   ],
   "source": [
    "for num_layers in [1, 4, 8]:\n",
    "    train_GPT_Othello(game_dataset, n_layers=num_layers, n_heads=1, batch_size=512, learning_rate=1e-4, lr_schedule=False, num_epochs=5, train_to=120000,val_legal_stepsize=1000, save_at_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent_world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
